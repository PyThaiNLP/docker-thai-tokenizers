import load_model

def tokeniser(sentence, method):
    return deepcut.tokenize(sentence)
